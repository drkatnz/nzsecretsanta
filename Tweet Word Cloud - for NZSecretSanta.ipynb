{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Word Cloud\n",
    "\n",
    "Gets the tweets from a given user, and analyses their most frequent words and produces a word cloud.\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "Python 3 (you can use Anaconda to get this - see https://www.anaconda.com/download/) \n",
    "\n",
    "- pandas\n",
    "- nltk\n",
    "- wordcloud\n",
    "- tweepy\n",
    "\n",
    "You can ```pip install``` all of these. Furthermore, nltk requires you to ```nltk.download()``` the popular data in order for this to run.\n",
    "\n",
    "You will need to create an application through https://apps.twitter.com/ to get the twitter API credentials.\n",
    "\n",
    "---\n",
    "\n",
    "Edit the cell below, then run all the cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CHANGE THESE\n",
    "\n",
    "twitter_handle = 'nzsecretsanta'\n",
    "\n",
    "#Twitter API credentials\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_key = \"\"\n",
    "access_secret = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NO TOUCHIE\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "% matplotlib inline\n",
    "\n",
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from time import sleep\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "def get_all_tweets(screen_name, alltweets=[], max_id=0):\n",
    "    #Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    if max_id is 0:\n",
    "        new_tweets = api.user_timeline(screen_name=screen_name, count=200, tweet_mode='extended')\n",
    "    else:\n",
    "        # new new_tweets\n",
    "        new_tweets = api.user_timeline(screen_name=screen_name, count= 200, max_id=max_id, tweet_mode='extended')\n",
    "\n",
    "    if len(new_tweets) > 0:\n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "        # security\n",
    "        sleep(2)\n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "        return get_all_tweets(screen_name=screen_name, alltweets=alltweets, max_id=oldest)\n",
    "\n",
    "    #final tweets\n",
    "    return alltweets\n",
    "\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    cleaned_text = re.sub(r'http[s]?:\\/\\/.*[\\W]*', '', tweet, flags=re.MULTILINE) # remove urls\n",
    "    cleaned_text = re.sub(r'@[\\w]*', '', cleaned_text, flags=re.MULTILINE) # remove the @twitter mentions \n",
    "    cleaned_text = re.sub(r'#[\\w]*', '', cleaned_text, flags=re.MULTILINE) # remove the hashtags\n",
    "    cleaned_text = re.sub(r'RT.*','', cleaned_text, flags=re.MULTILINE) # delete the retweets\n",
    "    cleaned_text = re.sub(r'[\\']','',cleaned_text, flags=re.MULTILINE)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def tweet_to_tokens(tweet):\n",
    "    #turns tweet text into a list of words\n",
    "    stop = stopwords.words('english') + list(string.punctuation) + ['`','\\'\\'','...','``']\n",
    "    return [i for i in word_tokenize(tweet.lower()) if i not in stop]\n",
    "\n",
    "\n",
    "def make_cloud_from_text(text, max_words=30):\n",
    "    # Generate a word cloud image\n",
    "    wordcloud = WordCloud(max_words=max_words).generate(text)    \n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # lower max_font_size\n",
    "    wordcloud = WordCloud(max_font_size=40,max_words=max_words).generate(text)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get all the tweets\n",
    "my_tweets = get_all_tweets(twitter_handle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make word clouds based on all words\n",
    "list_of_words = []\n",
    "for tweet in my_tweets:\n",
    "    list_of_words += (tweet_to_tokens(clean_tweet(tweet.full_text)))\n",
    "\n",
    "make_cloud_from_text(' '.join(list_of_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make word clouds just for nouns\n",
    "\n",
    "#parts of speech tagging\n",
    "list_of_nouns = []\n",
    "for tweet in my_tweets:\n",
    "    clean = clean_tweet(tweet.full_text) #clean urls etc off\n",
    "    no_punc = ''.join([i.lower() for i in clean if i not in string.punctuation]) #remove punctuation\n",
    "    tags = nltk.pos_tag(nltk.word_tokenize(no_punc)) #tag the text (requires sequence)\n",
    "    nouns = [i[0] for i in tags if i[1].startswith('N')] #reduce to just the nouns\n",
    "    nouns_no_plural = [i if not i.endswith('s') else i[:-1] for i in nouns] #remove plurals (more matches)\n",
    "    list_of_nouns += nouns_no_plural #add to list\n",
    "\n",
    "\n",
    "make_cloud_from_text(' '.join(list_of_nouns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
